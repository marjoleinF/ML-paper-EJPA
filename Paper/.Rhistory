plot(ct3, gp = gpar(cex = .5), ip_args = list(pval = FALSE))
ct_preds_train_s <- predict(ct_s, type = "prob")[ , "psychology"]
ct_preds_test_s <- predict(ct_s, newdata = data[test_ids , ], type = "prob")[ , "psychology"]
ct_form <- formula(paste("major ~", paste(varnames_i, collapse = "+")))
ct <- ctree(ct_form, data = data[train_ids , ], maxdepth = 6)
# myfun <- function(i) {c(
#   paste("n =", i$n),
#   format(round(i$distribution[2]/i$n, digits = 3), nsmall = 3)
# )}
#ct2 <- as.simpleparty(ct)
#plot(ct2, tp = node_terminal, tp_args = list(FUN = myfun), gp = gpar(cex = .5))
ct_preds_train_i <- predict(ct, type = "prob")[ , "psychology"]
ct_preds_test_i <- predict(ct, newdata = data[test_ids , ], type = "prob")[ , "psychology"]
library("gbm")
load("GBM_subscales.Rda")
gb_preds_train_s <- predict(gb_s, newdata = data[train_ids, ], type = "response")
gb_preds_test_s <- predict(gb_s, newdata = data[test_ids, ], type = "response")
library("pre")
load("PRE_subscales.Rda")
pr_preds_train_s <- predict(pr_s, type = "response")
pr_preds_test_s <- predict(pr_s, newdata = data[test_ids , ], type = "response")
imps <- pre::importance(pr_s, plot=FALSE)
varimps_pre <- imps$varimps
imps <- imps$baseimps[1:8, c("description", "coefficient")]
imps$coefficient <- round(imps$coefficient, digits = 3)
print(imps, row.names = FALSE)
library("class")
load("knn_preds_train_scales.Rda")
load("knn_preds_test_scales.Rda")
load("knn_preds_train_item.Rda")
load("knn_preds_test_item.Rda")
par(mfrow = c(1, 3))
## Logistic regression
glmod_s <- glm(major ~ Real + Inve + Arti + Soci + Ente + Conv,
data = data[train_ids , ], family = "binomial")
#summary(glmod_s)
sum <- as.data.frame(summary(glmod_s)$coefficients)
## default par(mar = c(5, 4, 4, 2) + 0.1) with c(bottom, left, top, right) margins
## default par(mgp = c(3, 1, 0) with c(axis title, axis labels, axis line) margins
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
library("colorspace")
plot(coef(glmod_s)[-1], xaxt = "n", ylab = "Estimated coefficient",
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(coef(glmod_s)[-1], labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
abline(0, 0, col = "grey")
#axis(1, 3.5 + c(0:5)*8, labels = c("Realistic", "Investigative", "Artistic", "Social" ,
#                             "Enterprising", "Conventional"),
#     cex.axis = .7)
legend("topleft", legend = "LR", cex = 1, bty = "n", text.font = 2)
glm_preds_train_s <- predict(glmod_s, newdata = data[train_ids, ], type = "response")
glm_preds_test_s <- predict(glmod_s, newdata = data[test_ids, ], type = "response")
## Generalized additive model
sum <- summary(gamod_s)
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(sum$chi.sq), xaxt = "n", ylab = expression(sqrt(chi^2)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(sum$chi.sq), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "GAM", cex = .7, bty = "n")
## Conditional inference tree
ct6 <- cforest(major ~ Real + Inve + Arti + Soci + Ente + Conv,
data = data[train_ids , ], ntree = 1L, mtry = 6,
perturb = list(replace = FALSE, fraction = 1L),
control = ctree_control(maxdepth = 6))
imps <- varimp(gettree(ct6), risk = "loglik")
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "Tree", cex = .7, bty = "n")
## Gradient boosted ensemble
sum <- summary(gb_s, plotit = FALSE, method = permutation.test.gbm)
imps <- sum$rel.inf
names(imps) <- sum$var
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "GB", cex = .7, bty = "n")
## Random forest
library("ranger")
load(file = "RF_subscales.Rda")
rf_preds_train_s <- predict(rf_s, data = data[train_ids, ])$predictions[ , "psychology"]
rf_preds_test_s <- predict(rf_s, data = data[test_ids, ])$predictions[ , "psychology"]
imps <- ranger::importance(rf_s)
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "RF", cex = .7, bty = "n")
## Prediction rule ensemble
imps <- varimps_pre$imp
names(imps) <- varimps_pre$varname
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "PRE", cex = .7, bty = "n", text.font = 2)
par(mfrow = c(1, 3))
## Logistic regression
glmod_s <- glm(major ~ Real + Inve + Arti + Soci + Ente + Conv,
data = data[train_ids , ], family = "binomial")
#summary(glmod_s)
sum <- as.data.frame(summary(glmod_s)$coefficients)
## default par(mar = c(5, 4, 4, 2) + 0.1) with c(bottom, left, top, right) margins
## default par(mgp = c(3, 1, 0) with c(axis title, axis labels, axis line) margins
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
library("colorspace")
plot(coef(glmod_s)[-1], xaxt = "n", ylab = "Estimated coefficient",
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(coef(glmod_s)[-1], labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
abline(0, 0, col = "grey")
#axis(1, 3.5 + c(0:5)*8, labels = c("Realistic", "Investigative", "Artistic", "Social" ,
#                             "Enterprising", "Conventional"),
#     cex.axis = .7)
legend("topleft", legend = "LR", cex = 1, bty = "n", text.font = 2)
glm_preds_train_s <- predict(glmod_s, newdata = data[train_ids, ], type = "response")
glm_preds_test_s <- predict(glmod_s, newdata = data[test_ids, ], type = "response")
## Generalized additive model
sum <- summary(gamod_s)
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(sum$chi.sq), xaxt = "n", ylab = expression(sqrt(chi^2)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(sum$chi.sq), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "GAM", cex = .7, bty = "n")
## Conditional inference tree
ct6 <- cforest(major ~ Real + Inve + Arti + Soci + Ente + Conv,
data = data[train_ids , ], ntree = 1L, mtry = 6,
perturb = list(replace = FALSE, fraction = 1L),
control = ctree_control(maxdepth = 6))
imps <- varimp(gettree(ct6), risk = "loglik")
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "Tree", cex = .7, bty = "n")
## Gradient boosted ensemble
sum <- summary(gb_s, plotit = FALSE, method = permutation.test.gbm)
imps <- sum$rel.inf
names(imps) <- sum$var
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "GB", cex = .7, bty = "n")
## Random forest
library("ranger")
load(file = "RF_subscales.Rda")
rf_preds_train_s <- predict(rf_s, data = data[train_ids, ])$predictions[ , "psychology"]
rf_preds_test_s <- predict(rf_s, data = data[test_ids, ])$predictions[ , "psychology"]
imps <- ranger::importance(rf_s)
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "RF", cex = 1, bty = "n")
## Prediction rule ensemble
imps <- varimps_pre$imp
names(imps) <- varimps_pre$varname
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7, main = "Prediction rule ensemble")
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
cex.main = .7, main = "Prediction rule ensemble")
text(sqrt(imps), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "PRE", cex = 1, bty = "n", text.font = 2)
par(mfrow = c(1, 3))
## Logistic regression
glmod_s <- glm(major ~ Real + Inve + Arti + Soci + Ente + Conv,
data = data[train_ids , ], family = "binomial")
#summary(glmod_s)
sum <- as.data.frame(summary(glmod_s)$coefficients)
## default par(mar = c(5, 4, 4, 2) + 0.1) with c(bottom, left, top, right) margins
## default par(mgp = c(3, 1, 0) with c(axis title, axis labels, axis line) margins
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
library("colorspace")
plot(coef(glmod_s)[-1], xaxt = "n", ylab = "Estimated coefficient",
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Logistic Regression", cex.main = .7)
text(coef(glmod_s)[-1], labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
abline(0, 0, col = "grey")
#axis(1, 3.5 + c(0:5)*8, labels = c("Realistic", "Investigative", "Artistic", "Social" ,
#                             "Enterprising", "Conventional"),
#     cex.axis = .7)
#legend("topleft", legend = "LR", cex = 1, bty = "n", text.font = 2)
glm_preds_train_s <- predict(glmod_s, newdata = data[train_ids, ], type = "response")
glm_preds_test_s <- predict(glmod_s, newdata = data[test_ids, ], type = "response")
## Generalized additive model
sum <- summary(gamod_s)
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(sum$chi.sq), xaxt = "n", ylab = expression(sqrt(chi^2)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Generalized Additive Model", cex.main = .7)
text(sqrt(sum$chi.sq), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "GAM", cex = .7, bty = "n")
## Conditional inference tree
ct6 <- cforest(major ~ Real + Inve + Arti + Soci + Ente + Conv,
data = data[train_ids , ], ntree = 1L, mtry = 6,
perturb = list(replace = FALSE, fraction = 1L),
control = ctree_control(maxdepth = 6))
imps <- varimp(gettree(ct6), risk = "loglik")
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Tree", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "Tree", cex = .7, bty = "n")
## Gradient boosted ensemble
sum <- summary(gb_s, plotit = FALSE, method = permutation.test.gbm)
imps <- sum$rel.inf
names(imps) <- sum$var
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Gradient Boosted Ensemble", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "GB", cex = .7, bty = "n")
## Random forest
library("ranger")
load(file = "RF_subscales.Rda")
rf_preds_train_s <- predict(rf_s, data = data[train_ids, ])$predictions[ , "psychology"]
rf_preds_test_s <- predict(rf_s, data = data[test_ids, ])$predictions[ , "psychology"]
imps <- ranger::importance(rf_s)
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Random Forest", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "RF", cex = 1, bty = "n")
## Prediction rule ensemble
imps <- varimps_pre$imp
names(imps) <- varimps_pre$varname
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
cex.main = .7, main = "Prediction Rule Ensemble")
text(sqrt(imps), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "PRE", cex = 1, bty = "n", text.font = 2)
par(mfrow = c(1, 3))
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
library("colorspace")
## Logistic regression
glmod_s <- glm(major ~ Real + Inve + Arti + Soci + Ente + Conv,
data = data[train_ids , ], family = "binomial")
#summary(glmod_s)
sum <- as.data.frame(summary(glmod_s)$coefficients)
## default par(mar = c(5, 4, 4, 2) + 0.1) with c(bottom, left, top, right) margins
## default par(mgp = c(3, 1, 0) with c(axis title, axis labels, axis line) margins
plot(coef(glmod_s)[-1], xaxt = "n", ylab = "Estimated coefficient",
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Logistic Regression", cex.main = .7)
text(coef(glmod_s)[-1], labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
abline(0, 0, col = "grey")
#axis(1, 3.5 + c(0:5)*8, labels = c("Realistic", "Investigative", "Artistic", "Social" ,
#                             "Enterprising", "Conventional"),
#     cex.axis = .7)
#legend("topleft", legend = "LR", cex = 1, bty = "n", text.font = 2)
glm_preds_train_s <- predict(glmod_s, newdata = data[train_ids, ], type = "response")
glm_preds_test_s <- predict(glmod_s, newdata = data[test_ids, ], type = "response")
## Generalized additive model
sum <- summary(gamod_s)
plot(sqrt(sum$chi.sq), xaxt = "n", ylab = expression(sqrt(chi^2)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Generalized Additive Model", cex.main = .7)
text(sqrt(sum$chi.sq), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "GAM", cex = .7, bty = "n")
## Conditional inference tree
ct6 <- cforest(major ~ Real + Inve + Arti + Soci + Ente + Conv,
data = data[train_ids , ], ntree = 1L, mtry = 6,
perturb = list(replace = FALSE, fraction = 1L),
control = ctree_control(maxdepth = 6))
imps <- varimp(gettree(ct6), risk = "loglik")
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Tree", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "Tree", cex = .7, bty = "n")
## Gradient boosted ensemble
sum <- summary(gb_s, plotit = FALSE, method = permutation.test.gbm)
imps <- sum$rel.inf
names(imps) <- sum$var
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Gradient Boosted Ensemble", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "GB", cex = .7, bty = "n")
## Random forest
library("ranger")
load(file = "RF_subscales.Rda")
rf_preds_train_s <- predict(rf_s, data = data[train_ids, ])$predictions[ , "psychology"]
rf_preds_test_s <- predict(rf_s, data = data[test_ids, ])$predictions[ , "psychology"]
imps <- ranger::importance(rf_s)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Random Forest", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "RF", cex = 1, bty = "n")
## Prediction rule ensemble
imps <- varimps_pre$imp
names(imps) <- varimps_pre$varname
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
cex.main = .7, main = "Prediction Rule Ensemble")
text(sqrt(imps), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "PRE", cex = 1, bty = "n", text.font = 2)
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
library("glmnet")
X <- as.matrix(data[train_ids, varnames_i])
set.seed(42)
glmod_i <- glmnet(X, train_y, family = "binomial", alpha = 1, lambda = 0.0003251157)
plot(coef(glmod_i)[-1], xaxt = "n", ylab = "Estimated coefficient",
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(coef(glmod_i)[-1], labels = varnames_i, cex = .5,
col = rep(qualitative_hcl(6), each = 8))
abline(0, 0, col = "grey")
legend("topleft", legend = "Penalized logistic regression", cex = .7, bty = "n")
glm_preds_train_i <- predict(glmod_i, newx = X, type = "response")
glm_preds_test_i <- predict(glmod_i, newx = as.matrix(data[test_ids, varnames_i]), type = "response")
library("mgcv")
load(file = "GAM_items.Rda")
sum <- summary(gamod_i)
plot(sqrt(sum$chi.sq), xaxt = "n", ylab = expression(sqrt(chi^2)),
main = " ", cex.main = .7,
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ")
text(sqrt(sum$chi.sq), labels = varnames_i, cex = .5,
col = rep(qualitative_hcl(6), each = 8))
legend("topleft", legend = "Generalized Additive Model", cex = .7, bty = "n")
gam_preds_train_i <- predict(gamod_i, newdata = data[train_ids, ], type = "response")
gam_preds_test_i <- predict(gamod_i, newdata = data[test_ids, ], type = "response")
ct6 <- cforest(ct_form,
data = data[train_ids , ], ntree = 1L, mtry = 6,
perturb = list(replace = FALSE, fraction = 1L),
control = ctree_control(maxdepth = 6))
imps <- varimp(gettree(ct6), risk = "loglik")
imp_names <- names(imps)
imps <- c(imps, rep(0, times = 48 - length(imps)))
names(imps) <- c(imp_names, varnames_i[!varnames_i %in% imp_names])
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps[varnames_i]), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(imps[varnames_i]), labels = varnames_i, cex = .5,
col = rep(qualitative_hcl(6), each = 8))
legend("topleft", legend = "Tree", cex = .7, bty = "n")
load(file = "GBM_items.Rda")
gb_preds_train_i <- predict(gb_i, newdata = data[train_ids, ], type = "response")
gb_preds_test_i <- predict(gb_i, newdata = data[test_ids, ], type = "response")
load(file = "gb_i_summary.Rda")
imps <- sum_i[match(varnames_i, sum_i$var), ]
plot(sqrt(imps$rel.inf), xaxt = "n", main = " ",
ylab = expression(sqrt(Importance)), cex.main = .7,
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ")
text(sqrt(imps$rel.inf), labels = imps$var, cex = .5,
col = rep(qualitative_hcl(6), each = 8))
legend("topleft", legend = "Boosted ensemble", cex = .7, bty = "n")
load("RF_items.Rda")
rf_preds_train_i <- predict(rf_i, data = data[train_ids, ])$predictions[ , "psychology"]
rf_preds_test_i <- predict(rf_i, data = data[test_ids, ])$predictions[ , "psychology"]
imps <- ranger::importance(rf_i)
plot(sqrt(imps), xaxt = "n", main = " ",
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
ylab = expression(sqrt(Importance)), cex.main = .7)
text(sqrt(imps), labels = names(imps), cex = .5,
col = rep(qualitative_hcl(6), each = 8))
legend("topleft", legend = "Random forest", cex = .7, bty = "n")
load("PRE_items.Rda")
pr_preds_train_i <- predict(pr_i, type = "response")
pr_preds_test_i <- predict(pr_i, newdata = data[test_ids , ], type = "response")
imps <- pre::importance(pr_i, cex.axis = .7, plot = FALSE)$varimps
zero_vars <- varnames_i[!varnames_i %in% imps[ , 1]]
imps <- rbind(imps, data.frame(varname = zero_vars,
imp = rep(0, times = length(zero_vars))))
imps <- imps[match(varnames_i, imps$varname), ]
plot(imps$imp, xaxt = "n", main = " ",
ylab = "Importance",
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ", cex.main = .7)
text(imps$imp, labels = imps$varname, cex = .5,
col = rep(qualitative_hcl(6), each = 8))
legend("topleft", legend = "Prediction rule ensemble", cex = .7, bty = "n")
1.75*6
devtools::install_github("rstudio/rmarkdown")
devtools::install_github("rstudio/rmarkdown")
library(rmarkdown)
detach("package:rmarkdown", unload = TRUE)
devtools::install_github("rstudio/rmarkdown")
install.packages("rmarkdown")
round(prop.table(table(train_y))[2]*(1-prop.table(table(test_y))[2]), digits = 4)
?ctree
### regression
airq <- subset(airquality, !is.na(Ozone))
airct <- ctree(Ozone ~ ., data = airq)
airct
plot(airct)
mean((airq$Ozone - predict(airct))^2)
library(partykit)
### regression
airq <- subset(airquality, !is.na(Ozone))
airct <- ctree(Ozone ~ ., data = airq)
airct
plot(airct)
mean((airq$Ozone - predict(airct))^2)
### regression
airq <- subset(airquality, !is.na(Ozone))
airct <- ctree(Ozone ~ ., data = airq, macdepth = 1)
airct
plot(airct)
mean((airq$Ozone - predict(airct))^2)
### regression
airq <- subset(airquality, !is.na(Ozone))
airct <- ctree(Ozone ~ ., data = airq, maxdepth = 1)
airct
plot(airct)
mean((airq$Ozone - predict(airct))^2)
### regression
airq <- subset(airquality, !is.na(Ozone))
airct <- ctree(Ozone ~ ., data = airq, maxdepth = 2)
airct
plot(airct)
mean((airq$Ozone - predict(airct))^2)
sessionInfo()
library("knitr")
knitr::opts_chunk$set(dpi=300)
library("mgcv")
load(file = "GAM_subscales.Rda")
par(mfrow = c(2, 3))
plot(gamod_s)
gam_preds_train_s <- predict(gamod_s, newdata = data[train_ids, ], type = "response")
library("partykit")
ct_s <- ctree(major ~ Real + Inve + Arti + Soci + Ente + Conv, data = data[train_ids , ],
maxdepth = 7)
ct_form <- formula(paste("major ~", paste(varnames_i, collapse = "+")))
x1 <- rnorm(100)
x2 <- x1 + rnorm(100)
x1 <- rnorm(100)
x2 <- x1 + rnorm(100)
y <- x1 + rnorm(100)
summary(lm(y ~ x1 + x2)
)
nreps <- 1000
coefs <- matrix(NA, ncol = nreps, nrow = 2)
for (i in 1:nreps) {
x1 <- rnorm(100)
x2 <- x1 + rnorm(100)
y <- x1 + rnorm(100)
coefs[i , ] <- coef(lm(y ~ x1 + x2))
}
nreps <- 1000
coefs <- matrix(NA, nrow = nreps, ncol = 2)
for (i in 1:nreps) {
x1 <- rnorm(100)
x2 <- x1 + rnorm(100)
y <- x1 + rnorm(100)
coefs[i , ] <- coef(lm(y ~ x1 + x2))
}
nreps <- 1000
coefs <- matrix(NA, nrow = nreps, ncol = 2)
for (i in 1:nreps) {
x1 <- rnorm(100)
x2 <- x1 + rnorm(100)
y <- x1 + rnorm(100)
coefs[i , ] <- coef(lm(y ~ x1 + x2))[-1]
}
colMeans(coefs)
