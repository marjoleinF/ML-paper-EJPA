col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(sum$chi.sq), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "Generalized Additive Model", cex = .7, bty = "n")
usm
sum
knitr::opts_chunk$set(dpi=300)
load("PRE_items.Rda")
par(mfrow = c(1, 3))
## Logistic regression
glmod_s <- glm(major ~ Real + Inve + Arti + Soci + Ente + Conv,
data = data[train_ids , ], family = "binomial")
knitr::opts_chunk$set(dpi=300)
#########################
##
## Prepare data
##
data <- read.delim("data.csv", header = TRUE)
## Items should be scored 1-5, 0 may be missings
data[ , 1:48][sapply(data[ , 1:48], function(x) x == 0)] <- NA
data <- data[complete.cases(data[ , 1:48]), ]
## Select only university students
data <- data[data$education >= 3, ]
## Recode response variable
psych_ids <- rowSums(sapply(c("psych", "psyhcology", "psycotherapy", "couns",
"behavior", "behaviour", "neuro"),
function(x) grepl(x, data$major, ignore.case = TRUE)))
data$major <- factor(ifelse(psych_ids > 0, "psychology", "other"))
## Create train and test sets
set.seed(42)
test_ids <- sample(1:nrow(data), ceiling(nrow(data)/4))
train_ids <- which(!1:nrow(data) %in% test_ids)
train_y <- as.numeric(data$major)[train_ids] - 1
test_y <- as.numeric(data$major)[test_ids] - 1
data$Real <- rowSums(data[ , paste0("R", 1:8)])
data$Inve <- rowSums(data[ , paste0("I", 1:8)])
data$Arti <- rowSums(data[ , paste0("A", 1:8)])
data$Soci <- rowSums(data[ , paste0("S", 1:8)])
data$Ente <- rowSums(data[ , paste0("E", 1:8)])
data$Conv <- rowSums(data[ , paste0("C", 1:8)])
# train_ids2 <- 1:ceiling(.75*nrow(data))
# test_ids2 <- which(!1:nrow(data) %in% train_ids2)
# train_y2 <- as.numeric(data$major)[train_ids2] - 1
# test_y2 <- as.numeric(data$major)[test_ids2] - 1
# par(mfrow = c(2, 3))
# for (i in c("Real", "Inve", "Arti", "Soci" , "Ente", "Conv")) {
#   dens <- density(data[ , i])
#   plot(dens, main = "", xlab = i)
#   polygon(dens, col="lightblue", border="black")
# }
# round(cor(data[ , c("Real", "Inve", "Arti", "Soci" , "Ente", "Conv")]), digits = 3L)
library("pROC")
library("xtable")
# tab <- xtable(cor(data[ , c("Real", "Inve", "Arti", "Soci" , "Ente", "Conv")]),
#        caption = "Correlations between potential predictor variables.",
#        align = c("r", "c", "c", "c", "c", "c", "c"), digits = 3L)
# print(tab, type = "html", file = "cor_table.html")
varnames_i <- paste0(rep(c("R", "I", "A", "S", "E", "C"), each = 8), 1:8)
varnames_s <- c("R", "I", "A", "S", "E", "C")
library("mgcv")
load(file = "GAM_subscales.Rda")
par(mfrow = c(2, 3))
plot(gamod_s)
gam_preds_train_s <- predict(gamod_s, newdata = data[train_ids, ], type = "response")
gam_preds_test_s <- predict(gamod_s, newdata = data[test_ids, ], type = "response")
library("partykit")
ct_s <- ctree(major ~ Real + Inve + Arti + Soci + Ente + Conv, data = data[train_ids , ],
maxdepth = 6)
#myfun <- function(i) {c(
#  paste("n =", i$n),
#  format(round(i$distribution[2]/i$n, digits = 3), nsmall = 3)
#)}
#ct2 <- as.simpleparty(ct)
#plot(ct2, tp = node_terminal, tp_args = list(FUN = myfun), gp = gpar(cex = .5))
ct3 <- ctree(major ~ Real + Inve + Arti + Soci + Ente + Conv, data = data[train_ids , ],
maxdepth = 3)
plot(ct3, gp = gpar(cex = .5), ip_args = list(pval = FALSE))
ct_preds_train_s <- predict(ct_s, type = "prob")[ , "psychology"]
ct_preds_test_s <- predict(ct_s, newdata = data[test_ids , ], type = "prob")[ , "psychology"]
ct_form <- formula(paste("major ~", paste(varnames_i, collapse = "+")))
ct <- ctree(ct_form, data = data[train_ids , ], maxdepth = 6)
# myfun <- function(i) {c(
#   paste("n =", i$n),
#   format(round(i$distribution[2]/i$n, digits = 3), nsmall = 3)
# )}
#ct2 <- as.simpleparty(ct)
#plot(ct2, tp = node_terminal, tp_args = list(FUN = myfun), gp = gpar(cex = .5))
ct_preds_train_i <- predict(ct, type = "prob")[ , "psychology"]
ct_preds_test_i <- predict(ct, newdata = data[test_ids , ], type = "prob")[ , "psychology"]
library("gbm")
load("GBM_subscales.Rda")
gb_preds_train_s <- predict(gb_s, newdata = data[train_ids, ], type = "response")
gb_preds_test_s <- predict(gb_s, newdata = data[test_ids, ], type = "response")
library("pre")
load("PRE_subscales.Rda")
pr_preds_train_s <- predict(pr_s, type = "response")
pr_preds_test_s <- predict(pr_s, newdata = data[test_ids , ], type = "response")
imps <- pre::importance(pr_s, plot=FALSE)
varimps_pre <- imps$varimps
imps <- imps$baseimps[1:8, c("description", "coefficient")]
imps$coefficient <- round(imps$coefficient, digits = 3)
print(imps, row.names = FALSE)
library("class")
load("knn_preds_train_scales.Rda")
load("knn_preds_test_scales.Rda")
load("knn_preds_train_item.Rda")
load("knn_preds_test_item.Rda")
par(mfrow = c(1, 3))
## Logistic regression
glmod_s <- glm(major ~ Real + Inve + Arti + Soci + Ente + Conv,
data = data[train_ids , ], family = "binomial")
#summary(glmod_s)
sum <- as.data.frame(summary(glmod_s)$coefficients)
## default par(mar = c(5, 4, 4, 2) + 0.1) with c(bottom, left, top, right) margins
## default par(mgp = c(3, 1, 0) with c(axis title, axis labels, axis line) margins
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
library("colorspace")
plot(coef(glmod_s)[-1], xaxt = "n", ylab = "Estimated coefficient",
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(coef(glmod_s)[-1], labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
abline(0, 0, col = "grey")
#axis(1, 3.5 + c(0:5)*8, labels = c("Realistic", "Investigative", "Artistic", "Social" ,
#                             "Enterprising", "Conventional"),
#     cex.axis = .7)
legend("topleft", legend = "LR", cex = 1, bty = "n", text.font = 2)
glm_preds_train_s <- predict(glmod_s, newdata = data[train_ids, ], type = "response")
glm_preds_test_s <- predict(glmod_s, newdata = data[test_ids, ], type = "response")
## Generalized additive model
sum <- summary(gamod_s)
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(sum$chi.sq), xaxt = "n", ylab = expression(sqrt(chi^2)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(sum$chi.sq), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "GAM", cex = .7, bty = "n")
## Conditional inference tree
ct6 <- cforest(major ~ Real + Inve + Arti + Soci + Ente + Conv,
data = data[train_ids , ], ntree = 1L, mtry = 6,
perturb = list(replace = FALSE, fraction = 1L),
control = ctree_control(maxdepth = 6))
imps <- varimp(gettree(ct6), risk = "loglik")
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "Tree", cex = .7, bty = "n")
## Gradient boosted ensemble
sum <- summary(gb_s, plotit = FALSE, method = permutation.test.gbm)
imps <- sum$rel.inf
names(imps) <- sum$var
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "GB", cex = .7, bty = "n")
## Random forest
library("ranger")
load(file = "RF_subscales.Rda")
rf_preds_train_s <- predict(rf_s, data = data[train_ids, ])$predictions[ , "psychology"]
rf_preds_test_s <- predict(rf_s, data = data[test_ids, ])$predictions[ , "psychology"]
imps <- ranger::importance(rf_s)
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "RF", cex = .7, bty = "n")
## Prediction rule ensemble
imps <- varimps_pre$imp
names(imps) <- varimps_pre$varname
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "PRE", cex = .7, bty = "n", text.font = 2)
par(mfrow = c(1, 3))
## Logistic regression
glmod_s <- glm(major ~ Real + Inve + Arti + Soci + Ente + Conv,
data = data[train_ids , ], family = "binomial")
#summary(glmod_s)
sum <- as.data.frame(summary(glmod_s)$coefficients)
## default par(mar = c(5, 4, 4, 2) + 0.1) with c(bottom, left, top, right) margins
## default par(mgp = c(3, 1, 0) with c(axis title, axis labels, axis line) margins
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
library("colorspace")
plot(coef(glmod_s)[-1], xaxt = "n", ylab = "Estimated coefficient",
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(coef(glmod_s)[-1], labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
abline(0, 0, col = "grey")
#axis(1, 3.5 + c(0:5)*8, labels = c("Realistic", "Investigative", "Artistic", "Social" ,
#                             "Enterprising", "Conventional"),
#     cex.axis = .7)
legend("topleft", legend = "LR", cex = 1, bty = "n", text.font = 2)
glm_preds_train_s <- predict(glmod_s, newdata = data[train_ids, ], type = "response")
glm_preds_test_s <- predict(glmod_s, newdata = data[test_ids, ], type = "response")
## Generalized additive model
sum <- summary(gamod_s)
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(sum$chi.sq), xaxt = "n", ylab = expression(sqrt(chi^2)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(sum$chi.sq), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "GAM", cex = .7, bty = "n")
## Conditional inference tree
ct6 <- cforest(major ~ Real + Inve + Arti + Soci + Ente + Conv,
data = data[train_ids , ], ntree = 1L, mtry = 6,
perturb = list(replace = FALSE, fraction = 1L),
control = ctree_control(maxdepth = 6))
imps <- varimp(gettree(ct6), risk = "loglik")
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "Tree", cex = .7, bty = "n")
## Gradient boosted ensemble
sum <- summary(gb_s, plotit = FALSE, method = permutation.test.gbm)
imps <- sum$rel.inf
names(imps) <- sum$var
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "GB", cex = .7, bty = "n")
## Random forest
library("ranger")
load(file = "RF_subscales.Rda")
rf_preds_train_s <- predict(rf_s, data = data[train_ids, ])$predictions[ , "psychology"]
rf_preds_test_s <- predict(rf_s, data = data[test_ids, ])$predictions[ , "psychology"]
imps <- ranger::importance(rf_s)
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
legend("topleft", legend = "RF", cex = 1, bty = "n")
## Prediction rule ensemble
imps <- varimps_pre$imp
names(imps) <- varimps_pre$varname
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7, main = "Prediction rule ensemble")
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
cex.main = .7, main = "Prediction rule ensemble")
text(sqrt(imps), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "PRE", cex = 1, bty = "n", text.font = 2)
par(mfrow = c(1, 3))
## Logistic regression
glmod_s <- glm(major ~ Real + Inve + Arti + Soci + Ente + Conv,
data = data[train_ids , ], family = "binomial")
#summary(glmod_s)
sum <- as.data.frame(summary(glmod_s)$coefficients)
## default par(mar = c(5, 4, 4, 2) + 0.1) with c(bottom, left, top, right) margins
## default par(mgp = c(3, 1, 0) with c(axis title, axis labels, axis line) margins
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
library("colorspace")
plot(coef(glmod_s)[-1], xaxt = "n", ylab = "Estimated coefficient",
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Logistic Regression", cex.main = .7)
text(coef(glmod_s)[-1], labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
abline(0, 0, col = "grey")
#axis(1, 3.5 + c(0:5)*8, labels = c("Realistic", "Investigative", "Artistic", "Social" ,
#                             "Enterprising", "Conventional"),
#     cex.axis = .7)
#legend("topleft", legend = "LR", cex = 1, bty = "n", text.font = 2)
glm_preds_train_s <- predict(glmod_s, newdata = data[train_ids, ], type = "response")
glm_preds_test_s <- predict(glmod_s, newdata = data[test_ids, ], type = "response")
## Generalized additive model
sum <- summary(gamod_s)
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(sum$chi.sq), xaxt = "n", ylab = expression(sqrt(chi^2)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Generalized Additive Model", cex.main = .7)
text(sqrt(sum$chi.sq), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "GAM", cex = .7, bty = "n")
## Conditional inference tree
ct6 <- cforest(major ~ Real + Inve + Arti + Soci + Ente + Conv,
data = data[train_ids , ], ntree = 1L, mtry = 6,
perturb = list(replace = FALSE, fraction = 1L),
control = ctree_control(maxdepth = 6))
imps <- varimp(gettree(ct6), risk = "loglik")
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Tree", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "Tree", cex = .7, bty = "n")
## Gradient boosted ensemble
sum <- summary(gb_s, plotit = FALSE, method = permutation.test.gbm)
imps <- sum$rel.inf
names(imps) <- sum$var
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Gradient Boosted Ensemble", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = .5,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "GB", cex = .7, bty = "n")
## Random forest
library("ranger")
load(file = "RF_subscales.Rda")
rf_preds_train_s <- predict(rf_s, data = data[train_ids, ])$predictions[ , "psychology"]
rf_preds_test_s <- predict(rf_s, data = data[test_ids, ])$predictions[ , "psychology"]
imps <- ranger::importance(rf_s)
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Random Forest", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "RF", cex = 1, bty = "n")
## Prediction rule ensemble
imps <- varimps_pre$imp
names(imps) <- varimps_pre$varname
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
cex.main = .7, main = "Prediction Rule Ensemble")
text(sqrt(imps), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "PRE", cex = 1, bty = "n", text.font = 2)
par(mfrow = c(1, 3))
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
library("colorspace")
## Logistic regression
glmod_s <- glm(major ~ Real + Inve + Arti + Soci + Ente + Conv,
data = data[train_ids , ], family = "binomial")
#summary(glmod_s)
sum <- as.data.frame(summary(glmod_s)$coefficients)
## default par(mar = c(5, 4, 4, 2) + 0.1) with c(bottom, left, top, right) margins
## default par(mgp = c(3, 1, 0) with c(axis title, axis labels, axis line) margins
plot(coef(glmod_s)[-1], xaxt = "n", ylab = "Estimated coefficient",
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Logistic Regression", cex.main = .7)
text(coef(glmod_s)[-1], labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
abline(0, 0, col = "grey")
#axis(1, 3.5 + c(0:5)*8, labels = c("Realistic", "Investigative", "Artistic", "Social" ,
#                             "Enterprising", "Conventional"),
#     cex.axis = .7)
#legend("topleft", legend = "LR", cex = 1, bty = "n", text.font = 2)
glm_preds_train_s <- predict(glmod_s, newdata = data[train_ids, ], type = "response")
glm_preds_test_s <- predict(glmod_s, newdata = data[test_ids, ], type = "response")
## Generalized additive model
sum <- summary(gamod_s)
plot(sqrt(sum$chi.sq), xaxt = "n", ylab = expression(sqrt(chi^2)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Generalized Additive Model", cex.main = .7)
text(sqrt(sum$chi.sq), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "GAM", cex = .7, bty = "n")
## Conditional inference tree
ct6 <- cforest(major ~ Real + Inve + Arti + Soci + Ente + Conv,
data = data[train_ids , ], ntree = 1L, mtry = 6,
perturb = list(replace = FALSE, fraction = 1L),
control = ctree_control(maxdepth = 6))
imps <- varimp(gettree(ct6), risk = "loglik")
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Tree", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "Tree", cex = .7, bty = "n")
## Gradient boosted ensemble
sum <- summary(gb_s, plotit = FALSE, method = permutation.test.gbm)
imps <- sum$rel.inf
names(imps) <- sum$var
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Gradient Boosted Ensemble", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "GB", cex = .7, bty = "n")
## Random forest
library("ranger")
load(file = "RF_subscales.Rda")
rf_preds_train_s <- predict(rf_s, data = data[train_ids, ])$predictions[ , "psychology"]
rf_preds_test_s <- predict(rf_s, data = data[test_ids, ])$predictions[ , "psychology"]
imps <- ranger::importance(rf_s)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = "Random Forest", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "RF", cex = 1, bty = "n")
## Prediction rule ensemble
imps <- varimps_pre$imp
names(imps) <- varimps_pre$varname
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
cex.main = .7, main = "Prediction Rule Ensemble")
text(sqrt(imps), labels = varnames_s, cex = 1,
col = rep(qualitative_hcl(6)), font = 2)
#legend("topleft", legend = "PRE", cex = 1, bty = "n", text.font = 2)
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
library("glmnet")
X <- as.matrix(data[train_ids, varnames_i])
set.seed(42)
glmod_i <- glmnet(X, train_y, family = "binomial", alpha = 1, lambda = 0.0003251157)
plot(coef(glmod_i)[-1], xaxt = "n", ylab = "Estimated coefficient",
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(coef(glmod_i)[-1], labels = varnames_i, cex = .5,
col = rep(qualitative_hcl(6), each = 8))
abline(0, 0, col = "grey")
legend("topleft", legend = "Penalized logistic regression", cex = .7, bty = "n")
glm_preds_train_i <- predict(glmod_i, newx = X, type = "response")
glm_preds_test_i <- predict(glmod_i, newx = as.matrix(data[test_ids, varnames_i]), type = "response")
library("mgcv")
load(file = "GAM_items.Rda")
sum <- summary(gamod_i)
plot(sqrt(sum$chi.sq), xaxt = "n", ylab = expression(sqrt(chi^2)),
main = " ", cex.main = .7,
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ")
text(sqrt(sum$chi.sq), labels = varnames_i, cex = .5,
col = rep(qualitative_hcl(6), each = 8))
legend("topleft", legend = "Generalized Additive Model", cex = .7, bty = "n")
gam_preds_train_i <- predict(gamod_i, newdata = data[train_ids, ], type = "response")
gam_preds_test_i <- predict(gamod_i, newdata = data[test_ids, ], type = "response")
ct6 <- cforest(ct_form,
data = data[train_ids , ], ntree = 1L, mtry = 6,
perturb = list(replace = FALSE, fraction = 1L),
control = ctree_control(maxdepth = 6))
imps <- varimp(gettree(ct6), risk = "loglik")
imp_names <- names(imps)
imps <- c(imps, rep(0, times = 48 - length(imps)))
names(imps) <- c(imp_names, varnames_i[!varnames_i %in% imp_names])
par(mar = c(2, 4, 2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
plot(sqrt(imps[varnames_i]), xaxt = "n", ylab = expression(sqrt(Importance)),
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
main = " ", cex.main = .7)
text(sqrt(imps[varnames_i]), labels = varnames_i, cex = .5,
col = rep(qualitative_hcl(6), each = 8))
legend("topleft", legend = "Tree", cex = .7, bty = "n")
load(file = "GBM_items.Rda")
gb_preds_train_i <- predict(gb_i, newdata = data[train_ids, ], type = "response")
gb_preds_test_i <- predict(gb_i, newdata = data[test_ids, ], type = "response")
load(file = "gb_i_summary.Rda")
imps <- sum_i[match(varnames_i, sum_i$var), ]
plot(sqrt(imps$rel.inf), xaxt = "n", main = " ",
ylab = expression(sqrt(Importance)), cex.main = .7,
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ")
text(sqrt(imps$rel.inf), labels = imps$var, cex = .5,
col = rep(qualitative_hcl(6), each = 8))
legend("topleft", legend = "Boosted ensemble", cex = .7, bty = "n")
load("RF_items.Rda")
rf_preds_train_i <- predict(rf_i, data = data[train_ids, ])$predictions[ , "psychology"]
rf_preds_test_i <- predict(rf_i, data = data[test_ids, ])$predictions[ , "psychology"]
imps <- ranger::importance(rf_i)
plot(sqrt(imps), xaxt = "n", main = " ",
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
ylab = expression(sqrt(Importance)), cex.main = .7)
text(sqrt(imps), labels = names(imps), cex = .5,
col = rep(qualitative_hcl(6), each = 8))
legend("topleft", legend = "Random forest", cex = .7, bty = "n")
load("PRE_items.Rda")
pr_preds_train_i <- predict(pr_i, type = "response")
pr_preds_test_i <- predict(pr_i, newdata = data[test_ids , ], type = "response")
imps <- pre::importance(pr_i, cex.axis = .7, plot = FALSE)$varimps
zero_vars <- varnames_i[!varnames_i %in% imps[ , 1]]
imps <- rbind(imps, data.frame(varname = zero_vars,
imp = rep(0, times = length(zero_vars))))
imps <- imps[match(varnames_i, imps$varname), ]
plot(imps$imp, xaxt = "n", main = " ",
ylab = "Importance",
col = "white", cex.lab = .7, cex.axis = .7, xlab = " ", cex.main = .7)
text(imps$imp, labels = imps$varname, cex = .5,
col = rep(qualitative_hcl(6), each = 8))
legend("topleft", legend = "Prediction rule ensemble", cex = .7, bty = "n")
1.75*6
devtools::install_github("rstudio/rmarkdown")
devtools::install_github("rstudio/rmarkdown")
library(rmarkdown)
detach("package:rmarkdown", unload = TRUE)
devtools::install_github("rstudio/rmarkdown")
install.packages("rmarkdown")
