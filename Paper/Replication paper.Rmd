---
title: "Replication scripts - Models fitted in main paper"
bibliography: paper.bib
biblio-style: "apalike"
csl: apa.csl
output: pdf_document
header-includes:
    - \usepackage{setspace}
---

\onehalfspacing


```{r setup, include=FALSE}
library("knitr")
knitr::opts_chunk$set(dpi=300)
```


## Method

All analyses were performed in **`R`** [version `r paste0(R.Version()$major, ".", R.Version()$minor)`, @R21] We fitted (penalized) logistic regression models using **`R`** package **``glmnet``** [version `r packageVersion("glmnet")`, @FrieyHast10]; generalized additive models (GAMs) with smoothing splines suing pacakge **``mgcv``** [version `r packageVersion("mgcv")`, @Wood17]; conditional inference trees using package **``partykit``** [version `r packageVersion("partykit")`, @HothyZeil15; @HothyHorn06]; gradient boosted tree ensembles using package **``gbm``** [version `r packageVersion("gbm")`, @GreeyBoeh20]; random forests using package **``ranger``** [version `r packageVersion("ranger")`, @WrigyZieg17]; prediction rule ensembles using package **``pre``** [version `r packageVersion("pre")`, @Fokk20]; $k$ nearest neighbours using package **``class``** [version `r packageVersion("class")`, @VeneyRipl02].

### Dataset

```{r}
data <- read.delim("data.csv", header = TRUE)

## Items should be scored 1-5, 0 may be missings
data[ , 1:48][sapply(data[ , 1:48], function(x) x == 0)] <- NA
data <- data[complete.cases(data[ , 1:48]), ]

## Select only university students
data <- data[data$education >= 3, ]
# table(data$major)
psych_ids <- rowSums(sapply(c("psych", "psyhcology", "psycotherapy", "couns", 
                              "behavior", "behaviour", "neuro"),
                            function(x) grepl(x, data$major, ignore.case = TRUE)))
anim_ids <- grepl("anim", data$major, ignore.case = TRUE) ## exclude animal psych
data$major <- factor(ifelse(psych_ids > 0, "psychology", "other"))
data$major[anim_ids > 0 & psych_ids > 0] <- "other"

set.seed(42)
test_ids <- sample(1:nrow(data), ceiling(nrow(data)/4))
train_ids <- which(!1:nrow(data) %in% test_ids)
train_y <- as.numeric(data$major)[train_ids] - 1
test_y <- as.numeric(data$major)[test_ids] - 1

data$Real <- rowSums(data[ , paste0("R", 1:8)])
data$Inve <- rowSums(data[ , paste0("I", 1:8)])
data$Arti <- rowSums(data[ , paste0("A", 1:8)])
data$Soci <- rowSums(data[ , paste0("S", 1:8)])
data$Ente <- rowSums(data[ , paste0("E", 1:8)])
data$Conv <- rowSums(data[ , paste0("C", 1:8)])
```

```{r}
format(nrow(data), nsmall = 0, big.mark = ",")
format(unname(round(100*table(data$major)[2] / nrow(data), digits = 2)), 
       nsmall = 0, big.mark = ",")
format(unname(round(100*table(data$major)[1] / nrow(data), digits = 2)), 
       nsmall = 0, big.mark = ",")
``` 


### Model fitting and evaluation

```{r} 
format(length(train_ids), nsmall = 0, big.mark = ",")
format(unname(round(100*table(data$major[train_ids])[2] / length(train_ids), 
                    digits = 2)), nsmall = 0, big.mark = ",")
format(length(test_ids), nsmall = 0, big.mark = ",")
format(unname(round(100*table(data$major[test_ids])[2] / length(test_ids), 
                    digits = 2)), nsmall = 0, big.mark = ",")
paste0(R.Version()$major, ".", R.Version()$minor)
```



## Results

```{r}
varnames_i <- paste0(rep(c("R", "I", "A", "S", "E", "C"), each = 8), 1:8)
varnames_s <- c("R", "I", "A", "S", "E", "C")
```

### (penalized) Logistic regression

```{r}
glmod_s <- glm(major ~ Real + Inve + Arti + Soci + Ente + Conv, 
               data = data[train_ids , ], family = "binomial")
#summary(glmod_s)
glm_preds_train_s <- predict(glmod_s, newdata = data[train_ids, ], type = "response")
glm_preds_test_s <- predict(glmod_s, newdata = data[test_ids, ], type = "response")
```

```{r, warning=FALSE, message=FALSE}
library("glmnet")
X <- as.matrix(data[train_ids, varnames_i])
set.seed(42)  
glmod_i <- glmnet(X, train_y, family = "binomial", alpha = 1, lambda = 0.0003568404)
glm_preds_train_i <- predict(glmod_i, newx = X, type = "response")
glm_preds_test_i <- predict(glmod_i, newx = as.matrix(data[test_ids, varnames_i]), 
                            type = "response")
```


### Generalized additive model

```{r, eval=FALSE, fig.height=6, fig.width=11, warning=FALSE, message=FALSE}
library("mgcv")
gamod_s <- gam(major ~ s(Real) + s(Inve) + s(Arti) + s(Soci) + s(Ente) + s(Conv), 
               data = data[train_ids , ], family = "binomial")
```

```{r, echo=FALSE, eval=FALSE}
save(gamod_s, file = "GAM_subscales.Rda")
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library("mgcv")
load(file = "GAM_subscales.Rda")
```

**Figure 1**
```{r fig.width=7, fig.height=4, warning=FALSE, message=FALSE}
par(mfrow = c(2, 3))
plot(gamod_s)
gam_preds_train_s <- predict(gamod_s, newdata = data[train_ids, ], type = "response")
gam_preds_test_s <- predict(gamod_s, newdata = data[test_ids, ], type = "response")
```


```{r, echo=FALSE, eval=FALSE, warning=FALSE, message=FALSE}
library("mgcv")
#apply(data[train_ids, varnames], 2, table) ## all items have 5 options
gam_form <- formula(paste0("major ~", paste0("s(", varnames, ", k=4)", collapse = " + ")))
memory.limit(size = 32000)
```

```{r eval=FALSE}
gamod_i <- gam(gam_form, data = data[train_ids , ], family = "binomial", 
               select = TRUE)
```

```{r echo=FALSE, eval=FALSE}
save(gamod_i, file = "GAM_items.Rda")
```

```{r, echo=FALSE}
load("GAM_items.Rda")
```

```{r}
gam_preds_train_i <- predict(gamod_i, newdata = data[train_ids, ], type = "response")
gam_preds_test_i <- predict(gamod_i, newdata = data[test_ids, ], type = "response")
```




### Decision tree

**Figure 2**  
```{r, fig.height=3.5, fig.width=6.5, warning=FALSE, message=FALSE}
library("partykit")
ct_s <- ctree(major ~ Real + Inve + Arti + Soci + Ente + Conv, data = data[train_ids , ], 
              maxdepth = 7)
ct3 <- ctree(major ~ Real + Inve + Arti + Soci + Ente + Conv, data = data[train_ids , ], 
             maxdepth = 3)
plot(ct3, gp = gpar(cex = .5), ip_args = list(pval = FALSE))
ct_preds_train_s <- predict(ct_s, type = "prob")[ , "psychology"]
ct_preds_test_s <- predict(ct_s, newdata = data[test_ids , ], type = "prob")[ , "psychology"]
```

```{r, warning=FALSE, message=FALSE}
ct_form <- formula(paste("major ~", paste(varnames_i, collapse = "+")))
ct <- ctree(ct_form, data = data[train_ids , ], maxdepth = 9)
ct_preds_train_i <- predict(ct, type = "prob")[ , "psychology"]
ct_preds_test_i <- predict(ct, newdata = data[test_ids , ], type = "prob")[ , "psychology"]
```





### Gradient boosted tree ensemble

```{r, eval=FALSE, warning=FALSE, message = FALSE}
library("gbm")
set.seed(42)
gb_s <- gbm(I(as.numeric(major)-1) ~ Real + Inve + Arti + Soci + Ente + Conv, 
            n.trees = 1100, interaction.depth = 3L, shrinkage = 0.01, 
            data = data[train_ids , ])
```

```{r, echo=FALSE, eval=FALSE}
save(gb_s, file = "GBM_subscales.Rda")
```


```{r, echo=FALSE, warning=FALSE, message = FALSE}
library("gbm")
load("GBM_subscales.Rda")
```

```{r}
gb_preds_train_s <- predict(gb_s, newdata = data[train_ids, ], type = "response")
gb_preds_test_s <- predict(gb_s, newdata = data[test_ids, ], type = "response")
```


```{r, eval=FALSE, warning=FALSE, message=FALSE}
library("gbm")
set.seed(42)
gbm_form <- formula(paste("I(as.numeric(major)-1) ~ ", 
                          paste(paste0(rep(c("R", "I", "A", "S", "E", "C"), 
                                           each = 8), 1:8), collapse = "+")))
gb_i <- gbm(gbm_form, n.trees = 3500, interaction.depth = 5L, shrinkage = 0.01, 
            data = data[train_ids , ])
sum_i <- summary(gb_i, plotit = FALSE, method = permutation.test.gbm)
```

```{r echo=FALSE, eval=FALSE}
save(gb_i, file = "GBM_items.Rda")
save(sum_i, file = "gb_i_summary.Rda")
```


```{r, echo=FALSE, wanring=FALSE, message=FALSE}
load("GBM_items.Rda")
```

```{r}
gb_preds_train_i <- predict(gb_i, newdata = data[train_ids, ], type = "response")
gb_preds_test_i <- predict(gb_i, newdata = data[test_ids, ], type = "response")
```

### Random forest

```{r eval=FALSE, warning=FALSE, message=FALSE}
library("ranger")
set.seed(42)
rf_s <- ranger(major ~ Real + Inve + Arti + Soci + Ente + Conv, data = data[train_ids , ],
                   probability = TRUE, mtry = 3L, min.node.size = 500, 
             importance = "permutation")
```

```{r echo=FALSE, eval=FALSE}
save(rf_s, file = "RF_subscales.Rda")
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
library("ranger")
load("RF_subscales.Rda")
```

```{r}
rf_preds_train_s <- predict(rf_s, data = data[train_ids, ])$predictions[ , "psychology"]
rf_preds_test_s <- predict(rf_s, data = data[test_ids, ])$predictions[ , "psychology"]
```

```{r eval=FALSE, warning=FALSE, message=FALSE}
set.seed(42)
varnames <- paste0(rep(c("R", "I", "A", "S", "E", "C"), each = 8), 1:8)
rf_form <- formula(paste("major ~", paste(varnames, collapse = "+")))
rf_i <- ranger(rf_form, data = data[train_ids , ], probability = TRUE, 
             mtry = 10L, min.node.size = 500, importance = "permutation")
```

```{r eval=FALSE, echo=FALSE}
save(rf_i, file = "RF_items.Rda")
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
load("RF_items.Rda")
```

```{r}
rf_preds_train_i <- predict(rf_i, data = data[train_ids, ])$predictions[ , "psychology"]
rf_preds_test_i <- predict(rf_i, data = data[test_ids, ])$predictions[ , "psychology"]
```



### Prediction rule ensembling

```{r, eval=FALSE, echo=FALSE, warning=FALSE, message=FALSE}
library("pre")
set.seed(42)
pr_s <- pre(major ~ Real + Inve + Arti + Soci + Ente + Conv, data = data[train_ids , ], 
            learnrate = .05)
```

```{r echo=FALSE, eval= FALSE}
save(pr_s, file = "PRE_subscales.Rda")
```


```{r, echo=FALSE, warning=FALSE, message=FALSE}
library("pre")
load("PRE_subscales.Rda")
```

**Table 1**  
```{r}
pr_preds_train_s <- predict(pr_s, type = "response")
pr_preds_test_s <- predict(pr_s, newdata = data[test_ids , ], type = "response")
imps <- pre::importance(pr_s, plot=FALSE)
varimps_pre <- imps$varimps
imps <- imps$baseimps[1:6, c("description", "coefficient")]
colnames(imps) <- c("Description", "Coefficient")
imps$Coefficient <- round(imps$Coefficient, digits = 3)
kable(imps, row.names = FALSE, align = c("l", "c"))
``` 


```{r, eval=FALSE, echo=FALSE, warning=FALSE, message=FALSE}
pr_form <- formula(paste("major ~", paste(varnames_i, collapse = "+")))
set.seed(42)
pr_i <- pre(pr_form, data = data[train_ids , ], learnrate = .05,
            verbose = TRUE, family = "binomial")
```

```{r eval=FALSE, echo=FALSE}
save(pr_i, file = "PRE_items.Rda")
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
load("PRE_items.Rda")
```

```{r}
pr_preds_train_i <- predict(pr_i, type = "response")
pr_preds_test_i <- predict(pr_i, newdata = data[test_ids , ], type = "response")
```


 


### *k* Nearest neighbours

```{r, eval=FALSE}
library("class")

## Model for training predictions
knn_mod <- knn(train = data[train_ids , varnames_s], 
               test = data[train_ids , varnames_s], 
               cl = as.factor(data[train_ids, "major"]), 
               k = 300, use.all = TRUE, prob = TRUE)
## Need to obtain predicted probability for second class
knn_preds_train_s <- ifelse(
  knn_mod == "psychology", attr(knn_mod, "prob"), 1 - attr(knn_mod, "prob"))
```

```{r echo=FALSE, eval = FALSE}
save(knn_preds_train_s, file = "knn_preds_train_scales.Rda")
```

```{r, eval=FALSE}
## Model for testing predictions
knn_mod <- knn(train = data[train_ids , varnames_s], 
               test = data[test_ids , varnames_s], 
               cl = as.factor(data[train_ids, "major"]), 
               k = 300, use.all = TRUE, prob = TRUE)
knn_preds_test_s <- ifelse(
  knn_mod == "psychology", attr(knn_mod, "prob"), 1 - attr(knn_mod, "prob"))
```

```{r, eval=FALSE, echo=FALSE}
save(knn_preds_test_s, file = "knn_preds_test_scales.Rda")
```

```{r, echo=FALSE}
library("class")
```

```{r echo=FALSE}
load("knn_preds_train_scales.Rda")
load("knn_preds_test_scales.Rda")
```

```{r eval=FALSE}
## Model for training predictions
knn_mod <- knn(train = data[train_ids , varnames_i], 
               test = data[train_ids , varnames_i], 
               cl = as.factor(data[train_ids, "major"]), 
               k = 100, use.all = TRUE, prob = TRUE)
## Need to obtain predicted probability for second class
knn_preds_train_i <- ifelse(
  knn_mod == "psychology", attr(knn_mod, "prob"), 1 - attr(knn_mod, "prob"))
```

```{r eval=FALSE, echo=FALSE}
save(knn_preds_train_i, file = "knn_preds_train_item.Rda")
```

```{r eval=FALSE}
## Model for testing predictions
knn_mod <- knn(train = data[train_ids , varnames_i], 
               test = data[test_ids , varnames_i], 
               cl = as.factor(data[train_ids, "major"]), 
               k = 100, use.all = TRUE, prob = TRUE)
knn_preds_test <- ifelse(
  knn_mod == "psychology", attr(knn_mod, "prob"), 1 - attr(knn_mod, "prob"))
```

```{r echo=FALSE, eval=FALSE}
save(knn_preds_test_i, file = "knn_preds_test_item.Rda")
```

```{r, echo=FALSE}
load("knn_preds_train_item.Rda")
load("knn_preds_test_item.Rda")
```




## Model comparisons

### Variable contributions

**Figure 3**  

```{r, fig.width=7, fig.height=1.5, warning=FALSE, message=FALSE}
par(mfrow = c(1, 3))
par(mar = c(1, 4, 2, 1), mgp = c(1.5, .5, 0), tck = -0.05)
library("colorspace")

## Logistic regression
plot(coef(glmod_s)[-1], xaxt = "n", ylab = "Estimated coefficient",
     col = "white", cex.lab = .7, cex.axis = .7, xlab = " ", 
     main = "Logistic Regression", cex.main = .7)
text(coef(glmod_s)[-1], labels = varnames_s, cex = 1, 
     col = rep(qualitative_hcl(6)), font = 2)
abline(0, 0, col = "grey")

## Generalized additive model
sum <- summary(gamod_s)
plot(sqrt(sum$chi.sq), xaxt = "n", ylab = expression(sqrt(chi^2)),
     col = "white", cex.lab = .7, cex.axis = .7, xlab = " ", 
     main = "Generalized Additive Model", cex.main = .7)
text(sqrt(sum$chi.sq), labels = varnames_s, cex = 1, 
     col = rep(qualitative_hcl(6)), font = 2)

## Conditional inference tree
ct6 <- cforest(major ~ Real + Inve + Arti + Soci + Ente + Conv, 
               data = data[train_ids , ], ntree = 1L, mtry = 6,
               perturb = list(replace = FALSE, fraction = 1L),
               control = ctree_control(maxdepth = 6))
imps <- varimp(gettree(ct6), risk = "loglik")
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
     col = "white", cex.lab = .7, cex.axis = .7, xlab = " ", 
     main = "Tree", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = 1, 
     col = rep(qualitative_hcl(6)), font = 2)

## Gradient boosted ensemble
sum <- summary(gb_s, plotit = FALSE, method = permutation.test.gbm)
imps <- sum$rel.inf
names(imps) <- sum$var
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
     col = "white", cex.lab = .7, cex.axis = .7, xlab = " ", 
     main = "Gradient Boosted Ensemble", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = 1, 
     col = rep(qualitative_hcl(6)), font = 2)

## Random forest
library("ranger")
load(file = "RF_subscales.Rda")
imps <- ranger::importance(rf_s)
plot(sqrt(imps), xaxt = "n", ylab = expression(sqrt(Importance)),
     col = "white", cex.lab = .7, cex.axis = .7, xlab = " ", 
     main = "Random Forest", cex.main = .7)
text(sqrt(imps), labels = varnames_s, cex = 1, 
     col = rep(qualitative_hcl(6)), font = 2)

## Prediction rule ensemble
imps <- varimps_pre$imp
names(imps) <- varimps_pre$varname
imps <- imps[c("Real", "Inve", "Arti", "Soci", "Ente", "Conv")]
plot(imps, xaxt = "n", ylab = "Importance",
     col = "white", cex.lab = .7, cex.axis = .7, xlab = " ", 
     cex.main = .7, main = "Prediction Rule Ensemble")
text(imps, labels = varnames_s, cex = 1, 
     col = rep(qualitative_hcl(6)), font = 2)
```
  
  
**Figure 4**  
```{r, fig.width=6, fig.height=8, warning=FALSE, message=FALSE}
par(mar = c(1.5, 4, 0.2, 2), mgp = c(1.5, .5, 0), tck = -0.05)
par(mfrow = c(6, 1))

library("glmnet")
plot(coef(glmod_i)[-1], xaxt = "n", ylab = "Estimated coefficient",
     col = "white", cex.lab = .7, cex.axis = .7, xlab = " ", 
     main = " ", cex.main = .7)
text(coef(glmod_i)[-1], labels = varnames_i, cex = .5, 
     col = rep(qualitative_hcl(6), each = 8), font = 2)
abline(0, 0, col = "grey")
legend("topleft", legend = "Penalized logistic regression", cex = .7, bty = "n")
axis(1, 4.5 + c(0:5)*8, tick = FALSE, padj = -1.5,
     labels = c("Realistic", "Investigative", "Artistic", "Social" , 
                             "Enterprising", "Conventional"), 
     cex.axis = .7)

library("mgcv")
load(file = "GAM_items.Rda")
sum <- summary(gamod_i)
plot(sqrt(sum$chi.sq), xaxt = "n", ylab = expression(sqrt(chi^2)), 
     main = " ", cex.main = .7,
     col = "white", cex.lab = .7, cex.axis = .7, xlab = " ")
text(sqrt(sum$chi.sq), labels = varnames_i, cex = .5, 
     col = rep(qualitative_hcl(6), each = 8), font = 2)
legend("topleft", legend = "Generalized Additive Model", cex = .7, bty = "n")
axis(1, 4.5 + c(0:5)*8, tick = FALSE, padj = -1.5,
     labels = c("Realistic", "Investigative", "Artistic", "Social" , 
                             "Enterprising", "Conventional"), 
     cex.axis = .7)

ct6 <- cforest(ct_form, 
               data = data[train_ids , ], ntree = 1L, mtry = 6,
               perturb = list(replace = FALSE, fraction = 1L),
               control = ctree_control(maxdepth = 6))
imps <- varimp(gettree(ct6), risk = "loglik")
imp_names <- names(imps)
imps <- c(imps, rep(0, times = 48 - length(imps)))
names(imps) <- c(imp_names, varnames_i[!varnames_i %in% imp_names])
plot(sqrt(imps[varnames_i]), xaxt = "n", ylab = expression(sqrt(Importance)),
     col = "white", cex.lab = .7, cex.axis = .7, xlab = " ", 
     main = " ", cex.main = .7)
text(sqrt(imps[varnames_i]), labels = varnames_i, cex = .5, 
     col = rep(qualitative_hcl(6), each = 8), font = 2)
legend("topleft", legend = "Tree", cex = .7, bty = "n")
axis(1, 4.5 + c(0:5)*8, tick = FALSE, padj = -1.5,
     labels = c("Realistic", "Investigative", "Artistic", "Social" , 
                             "Enterprising", "Conventional"), 
     cex.axis = .7)

load(file = "gb_i_summary.Rda")
imps <- sum_i[match(varnames_i, sum_i$var), ]
plot(sqrt(imps$rel.inf), xaxt = "n", main = " ",
     ylab = expression(sqrt(Importance)), cex.main = .7,
     col = "white", cex.lab = .7, cex.axis = .7, xlab = " ")
text(sqrt(imps$rel.inf), labels = imps$var, cex = .5, 
     col = rep(qualitative_hcl(6), each = 8), font = 2)
legend("topleft", legend = "Boosted ensemble", cex = .7, bty = "n")
axis(1, 4.5 + c(0:5)*8, tick = FALSE, padj = -1.5,
     labels = c("Realistic", "Investigative", "Artistic", "Social" , 
                             "Enterprising", "Conventional"), 
     cex.axis = .7)

imps <- ranger::importance(rf_i)
plot(sqrt(imps), xaxt = "n", main = " ",
     col = "white", cex.lab = .7, cex.axis = .7, xlab = " ",
     ylab = expression(sqrt(Importance)), cex.main = .7)
text(sqrt(imps), labels = names(imps), cex = .5, 
     col = rep(qualitative_hcl(6), each = 8), font = 2)
legend("topleft", legend = "Random forest", cex = .7, bty = "n")
axis(1, 4.5 + c(0:5)*8, tick = FALSE, padj = -1.5,
     labels = c("Realistic", "Investigative", "Artistic", "Social" , 
                             "Enterprising", "Conventional"), 
     cex.axis = .7)

imps <- pre::importance(pr_i, cex.axis = .7, plot = FALSE)$varimps
zero_vars <- varnames_i[!varnames_i %in% imps[ , 1]]
imps <- rbind(imps, data.frame(varname = zero_vars, 
                               imp = rep(0, times = length(zero_vars))))
imps <- imps[match(varnames_i, imps$varname), ]
plot(imps$imp, xaxt = "n", main = " ",
     ylab = "Importance",
     col = "white", cex.lab = .7, cex.axis = .7, xlab = " ", cex.main = .7)
text(imps$imp, labels = imps$varname, cex = .5, 
     col = rep(qualitative_hcl(6), each = 8), font = 2)
legend("topleft", legend = "Prediction rule ensemble", cex = .7, bty = "n")
axis(1, 4.5 + c(0:5)*8, tick = FALSE, padj = -1.5,
     labels = c("Realistic", "Investigative", "Artistic", "Social" , 
                             "Enterprising", "Conventional"), 
     cex.axis = .7)
```
  
  
**Figure 5**  
```{r echo=FALSE, message = FALSE, warning=FALSE, fig.width=8, fig.height=3.5}
## Gather results for prediction with subscales
preds_train <- data.frame(bm_preds_train = rep(mean(train_y), times = length(train_ids)), 
                          glm_preds_train_s, gam_preds_train_s, 
                          ct_preds_train_s, pr_preds_train_s, gb_preds_train_s,
                          rf_preds_train_s, knn_preds_train_s)
preds_test <- data.frame(bm_preds_test = rep(mean(train_y), times = length(test_ids)),
                         glm_preds_test_s, gam_preds_test_s, 
                         ct_preds_test_s, pr_preds_test_s, gb_preds_test_s,
                         rf_preds_test_s, knn_preds_test_s)
results <- data.frame(
  Brier_train = sapply(preds_train, function(x) mean((train_y - x)^2)),
  Brier_test = sapply(preds_test, function(x) mean((test_y - x)^2)))
results$R2_train <- 1 - results$Brier_train / results$Brier_train[1]
results$R2_test <- 1 - results$Brier_test / results$Brier_test[1]
rownames(results) <- c("chance", "(penalized)\nGLM", "Smoothing\nsplines", 
                       "tree", "PRE", "Boosted\nensemble", "Random\nforest", "kNN")

tmp <- data.frame(
  R2 = c(results$R2_train[-1], results$R2_test[-1]),
  SE = c(sapply((preds_train[,-1] - train_y)^2, 
                function(x) sd(x)/(sqrt(nrow(preds_train))*results$Brier_train[1])),
         sapply((preds_test[,-1] - test_y)^2, 
                function(x) sd(x)/(sqrt(nrow(preds_test))*results$Brier_test[1]))),
  data = rep(c("train", "test"), each = nrow(results)-1), 
  method = rep(c("GLM", "GAM", "tree", "PRE", "GBE", "RF", "kNN"), times = 2))
tmp$data <- factor(tmp$data, levels = c("train", "test"))
tmp$method <- factor(tmp$method, levels = c("GLM", "GAM", "tree", "PRE", "GBE", "RF", "kNN"))

## Plot results
library("ggplot2")
p_subscales <- ggplot(tmp, aes(x = method, y = R2)) +
  geom_point(aes(color = data, fill = data), 
             stat = "identity", position = position_dodge(0.4)) + 
  ggtitle("subscales") + xlab("") + ylab("Pseudo R squared") + 
  theme(legend.position = "none", plot.title = element_text(size = 11)) +
  scale_color_manual(values = c("#0073C2FF", "#ff9933")) + ylim(0, 0.25) +
  scale_fill_manual(values = c("#0073C2FF", "#ff9933")) + 
  geom_errorbar(aes(color=data, ymin = R2-1.96*SE, ymax = R2+1.96*SE), 
                width = .2, position = position_dodge(0.4))

## Gather results for prediction with items
preds_train <- data.frame(bm_preds_train = rep(mean(train_y), times = length(train_ids)), 
                          glm_preds_train_i, gam_preds_train_i,
                          ct_preds_train_i, pr_preds_train_i, gb_preds_train_i,
                          rf_preds_train_i, knn_preds_train_i)
preds_test <- data.frame(bm_preds_test = rep(mean(train_y), times = length(test_ids)),
                          glm_preds_test_i, gam_preds_test_i,
                          ct_preds_test_i, pr_preds_test_i, gb_preds_test_i, 
                         rf_preds_test_i, knn_preds_test_i)
results <- data.frame(
  Brier_train = sapply(preds_train, function(x) mean((train_y - x)^2)),
  Brier_test = sapply(preds_test, function(x) mean((test_y - x)^2)))
results$R2_train <- 1 - results$Brier_train / results$Brier_train[1]
results$R2_test <- 1 - results$Brier_test / results$Brier_test[1]
rownames(results) <- c("chance", "(penalized)\nGLM", "Smoothing\nsplines (GAM)", 
                       "tree", "PRE", "Boosted\nensemble", "Random\nforest", "kNN")

tmp <- data.frame(
  R2 = c(results$R2_train[-1], results$R2_test[-1]),
  SE = c(sapply((preds_train[,-1] - train_y)^2, 
                function(x) sd(x)/(sqrt(nrow(preds_train))*results$Brier_train[1])),
          sapply((preds_test[,-1] - test_y)^2, 
                function(x) sd(x)/(sqrt(nrow(preds_test))*results$Brier_test[1]))),
  data = rep(c("train", "test"), each = nrow(results)-1), 
  method = rep(c("pGLM", "GAM", "tree", "PRE", "GBE", "RF", "kNN"), times = 2))
tmp$data <- factor(tmp$data, levels = c("train", "test"))
tmp$method <- factor(tmp$method, levels = c("pGLM", "GAM", "tree", "PRE", "GBE", "RF", "kNN"))

## Plot results
p_items <- ggplot(tmp, aes(x = method, y = R2)) +
  geom_point(aes(color = data, fill = data), 
    stat = "identity", position = position_dodge(0.4)) + xlab("") + ylab(" ") + 
  theme(axis.text.y=element_blank(), axis.ticks.y = element_blank(),
        plot.title = element_text(size = 11), legend.title = element_blank()) +
  ggtitle("items") + scale_color_manual(values = c("#0073C2FF", "#ff9933")) +
  scale_fill_manual(values = c("#0073C2FF", "#ff9933")) + ylim(0, .25) +
  geom_errorbar(aes(color=data, ymin = R2-1.96*SE, ymax = R2+1.96*SE), 
                width = .2, position = position_dodge(0.4))

library("gridExtra")
grid.arrange(p_subscales, p_items, ncol = 2, widths = c(6.75,8))
```



\newpage
# Appendix B: Uni- and bivariate sample descriptives

```{r}
data$age[data$age > 103] <- NA 
## some crazy ages were supplied
gender_tab <- prop.table(table(data$gender)) 
## 0=missing, 1=Male, 2=Female, 3=Other
marital_tab <- prop.table(table(data$married)) 
## 0=missing, 1=Never married, 2=Currently married, 3=Previously married
urban_tab <- prop.table(table(data$urban)) 
## 0=missing, 1=Rural (country side), 2=Suburban, 3=Urban (town, city)
race_tab <- prop.table(table(data$race)) 
## 0=missing, 1=Asian, 2=Arab, 3=Black, 
## 4=Indigenous Australian / Native American / White, 5=Other 
## (There was a coding mistake resulting in cat 4)
language_tab <- prop.table(table(data$engnat)) 
## 0=missing, 1=Yes, 2=No
```

```{r}
format(nrow(data), nsmall = 0, big.mark = ",") 
format(mean(data$age, na.rm = TRUE), digits = 2, nsmall = 2, big.mark = ",")
format(sd(data$age, na.rm = TRUE), nsmall = 2, digits = 2, big.mark = ",")
format(100*gender_tab[3], nsmall = 0, digits = 0, big.mark = ",")
format(100*gender_tab[2], nsmall = 0, digits = 0, big.mark = ",")
format(100*gender_tab[4], nsmall = 0, digits = 0, big.mark = ",")
format(100*marital_tab[2], nsmall = 0, digits = 0, big.mark = ",")
format(100*marital_tab[3], nsmall = 0, digits = 0, big.mark = ",")
format(100*marital_tab[4], nsmall = 0, digits = 0, big.mark = ",")
format(100*urban_tab[2], nsmall = 0, digits = 0, big.mark = ",")
format(100*urban_tab[3], nsmall = 0, digits = 0, big.mark = ",")
format(100*urban_tab[4], nsmall = 0, digits = 0, big.mark = ",")
format(100*language_tab[2], nsmall = 0, digits = 0, big.mark = ",")
format(100*language_tab[3], nsmall = 0, digits = 0, big.mark = ",")
format(100*race_tab[2], nsmall = 0, digits = 0, big.mark = ",")
format(100*race_tab[3], nsmall = 0, digits = 0, big.mark = ",")
format(100*race_tab[4], nsmall = 0, digits = 0, big.mark = ",")
format(100*race_tab[5], nsmall = 0, digits = 0, big.mark = ",")
format(100*race_tab[1], nsmall = 0, digits = 0, big.mark = ",")   
```

  
**Figure 6**  
```{r, warning=FALSE, message=FALSE, fig.width=8, fig.height=4.8}
par(mfrow = c(2, 3))
for (i in c("Real", "Inve", "Arti", "Soci" , "Ente", "Conv")) {
  dens <- density(data[ , i])
  plot(dens, main = "", xlab = i)
  polygon(dens, col="lightblue", border="black")  
}
```
  
  
  
  
**Table 2**  
```{r}
tab <- round(cor(data[ , c("Real", "Inve", "Arti", "Soci" , "Ente", "Conv")]), 
             digits = 3L)
kable(tab)
```


# References

